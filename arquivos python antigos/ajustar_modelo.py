import pandas as pd
import numpy as np
import rasterio
import os
import glob
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
from tqdm import tqdm
from geopy.distance import geodesic

def obter_variaveis_disponiveis():
    """Identifica todas as vari√°veis bioclim√°ticas dispon√≠veis em ambos cen√°rios"""
    cli_atual = set(os.path.basename(f) for f in glob.glob("clima_atual/*.tif"))
    cli_futuro = set(os.path.basename(f) for f in glob.glob("clima_futuro/clima_futuro2/*.tif"))
    comuns = sorted(list(cli_atual & cli_futuro))
    print(f"‚úÖ Vari√°veis em comum: {len(comuns)}")
    for var in comuns:
        print(f"  - {var}")

    # NOVO: Verifica se os arquivos futuros est√£o corretos e tem o mesmo formato
    try:
        with rasterio.open(os.path.join("clima_atual", comuns[0])) as src_atual:
            shape_atual = src_atual.shape
        with rasterio.open(os.path.join("clima_futuro", comuns[0])) as src_futuro:
            shape_futuro = src_futuro.shape
        
        if shape_atual != shape_futuro:
            print(f"‚ö†Ô∏è ALERTA: Arquivos atuais ({shape_atual}) e futuros ({shape_futuro}) t√™m dimens√µes diferentes!")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao verificar dimens√µes dos arquivos: {str(e)}")
    
    return comuns

def stack_rasters(raster_dir, selected_files):
    """Empilha m√∫ltiplos rasters em um √∫nico array 3D"""
    files = [os.path.join(raster_dir, f) for f in selected_files]
    arrays = []
    meta = None
    for file in tqdm(files, desc=f"üìö Empilhando rasters em {raster_dir}"):
        with rasterio.open(file) as src:
            if meta is None:
                meta = src.meta.copy()
            arrays.append(src.read(1))
    stacked = np.stack(arrays, axis=-1)
    return stacked, meta

def extract_values(coords, raster_stack, meta):
    """Extrai valores de raster para cada coordenada"""
    results = []
    for lat, lon in tqdm(coords, desc="üìå Extraindo valores"):
        row, col = rasterio.transform.rowcol(meta['transform'], lon, lat)
        if 0 <= row < raster_stack.shape[0] and 0 <= col < raster_stack.shape[1]:
            results.append(raster_stack[row, col])
        else:
            results.append([np.nan]*raster_stack.shape[2])
    return np.array(results)

def generate_pseudo_absences(clima, meta, pres_coords, n_samples, min_dist_km=30):
    """Gera pontos de pseudo-aus√™ncia longe das presen√ßas conhecidas"""
    absences = []
    attempts = 0
    max_attempts = n_samples * 30  # Aumentado para mais tentativas
    
    print(f"üéØ Gerando {n_samples} pseudo-aus√™ncias")
    with tqdm(total=n_samples, desc="üö´ Gerando pseudo-aus√™ncias") as pbar:
        while len(absences) < n_samples and attempts < max_attempts:
            # Estrat√©gia para melhor distribui√ß√£o espacial: dividir em quadrantes
            row = np.random.randint(0, clima.shape[0])
            col = np.random.randint(0, clima.shape[1])
            vals = clima[row, col]
            if np.isnan(vals).any():
                attempts += 1
                continue
            lon, lat = rasterio.transform.xy(meta['transform'], row, col)
            
            # Verifica dist√¢ncia m√≠nima das presen√ßas (agora 30km em vez de 20km)
            if all(geodesic((lat, lon), pc).km > min_dist_km for pc in pres_coords):
                absences.append(vals)
                pbar.update(1)
            attempts += 1
    
    return np.array(absences)

def ajustar_modelo():
    print("üîÑ Iniciando ajuste do modelo para reduzir overfitting...")
    
    # Obter vari√°veis dispon√≠veis
    variaveis = obter_variaveis_disponiveis()
    
    if len(variaveis) <= 2:
        print("‚ö†Ô∏è Apenas 2 vari√°veis bioclim√°ticas dispon√≠veis. Para melhor desempenho, adicione mais vari√°veis.")
    
    # Ler ocorr√™ncias
    print("üìä Lendo dados de ocorr√™ncia...")
    df = pd.read_csv("ocorrencias.csv")
    coords = list(zip(df["latitude"], df["longitude"]))
    print(f"‚úÖ Presen√ßas: {len(coords)}")
    
    # Empilhar rasters
    print("üìö Empilhando rasters...")
    clima_atual, meta_atual = stack_rasters("clima_atual", variaveis)
    print(f"‚úÖ Clima atual empilhado: {clima_atual.shape}")
    
    # Extrair valores para presen√ßas
    presen√ßas = extract_values(coords, clima_atual, meta_atual)
    
    # Remover presen√ßas com NaN
    nan_mask = np.isnan(presen√ßas).any(axis=1)
    if nan_mask.any():
        print(f"‚ö†Ô∏è Removendo {nan_mask.sum()} presen√ßas com valores NaN")
        presen√ßas = presen√ßas[~nan_mask]
        presenca_coords = [coord for i, coord in enumerate(coords) if not nan_mask[i]]
    else:
        presenca_coords = coords
    
    # Gerar pseudo-aus√™ncias
    n_ausencias = len(presen√ßas)
    ausencias = generate_pseudo_absences(clima_atual, meta_atual, presenca_coords, n_ausencias, min_dist_km=30)
    
    # Preparar dados para treinamento
    X = np.vstack((presen√ßas, ausencias))
    y = np.array([1]*len(presen√ßas) + [0]*len(ausencias))
    
    # Normalizar dados
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Dividir em treino/teste
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2)
    
    # Treinar modelo com par√¢metros para prevenir overfitting
    print("üîß Treinando modelo com par√¢metros para reduzir overfitting...")
    
    # Par√¢metros mais conservadores para evitar overfitting
    # - Aumentando min_samples_leaf ainda mais
    # - Limitando max_depth para evitar √°rvores muito profundas
    # - Reduzindo n√∫mero de √°rvores para evitar memoriza√ß√£o
    rf = RandomForestClassifier(
        n_estimators=50,          # Menos √°rvores (era 100)
        min_samples_leaf=30,      # Muito maior (era 10)
        max_depth=10,             # Mais limitado (era 15)
        max_features='sqrt',
        class_weight='balanced',
        random_state=42
    )
    
    rf.fit(X_train, y_train)
    
    # Avaliar modelo
    test_acc = rf.score(X_test, y_test)
    print(f"‚úÖ Acur√°cia no teste: {test_acc:.3f}")
    
    # Salvar modelo
    print("üíæ Salvando modelo e scaler...")
    joblib.dump(rf, "modelo_ajustado.pkl")
    joblib.dump(scaler, "scaler_ajustado.pkl")
    joblib.dump(variaveis, "variaveis_usadas.pkl")
    
    # Salvar lista de vari√°veis
    with open("variaveis_usadas.txt", "w") as f:
        for var in variaveis:
            f.write(f"{var}\n")
    
    print("‚úÖ Modelo ajustado! Para gerar os novos mapas, execute:")
    print("1. python gerar_mapas_ajustados.py")
    print("2. python mascarar_brasil.py")
    print("3. python visualizar_mapas.py")

if __name__ == "__main__":
    ajustar_modelo() 