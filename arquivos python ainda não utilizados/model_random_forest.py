import os
import numpy as np
import pandas as pd
import rasterio
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel
import joblib
import glob
from tqdm import tqdm
from geopy.distance import geodesic
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance

def stack_rasters(raster_dir, selected_files=None):
    all_files = sorted(glob.glob(os.path.join(raster_dir, "*.tif")))
    if selected_files:
        files = [f for f in all_files if os.path.basename(f) in selected_files]
    else:
        files = all_files
    arrays = []
    meta = None
    for file in files:
        with rasterio.open(file) as src:
            if meta is None:
                meta = src.meta.copy()
            arrays.append(src.read(1))
    stacked = np.stack(arrays, axis=-1)
    return stacked, meta, [os.path.basename(f) for f in files]

def extract_raster_values(coords, raster_stack, meta):
    results = []
    for lat, lon in tqdm(coords, desc="ðŸ“Œ Extraindo valores ambientais"):
        row, col = rasterio.transform.rowcol(meta['transform'], lon, lat)
        if 0 <= row < raster_stack.shape[0] and 0 <= col < raster_stack.shape[1]:
            results.append(raster_stack[row, col])
        else:
            results.append([np.nan]*raster_stack.shape[2])
    return np.array(results)

def generate_pseudo_absences(clima, meta, pres_coords, n_samples, min_dist_km=20):
    absences = []
    attempts = 0
    max_attempts = n_samples * 20
    
    print(f"ðŸŽ¯ Meta: gerar {n_samples} pseudo-ausÃªncias")
    with tqdm(total=n_samples, desc="ðŸš« Gerando pseudo-ausÃªncias") as pbar:
        while len(absences) < n_samples and attempts < max_attempts:
            row = np.random.randint(0, clima.shape[0])
            col = np.random.randint(0, clima.shape[1])
            vals = clima[row, col]
            if np.isnan(vals).any():
                attempts += 1
                continue
            lon, lat = rasterio.transform.xy(meta['transform'], row, col)
            if all(geodesic((lat, lon), pc).km > min_dist_km for pc in pres_coords):
                absences.append(vals)
                pbar.update(1)
            attempts += 1
            pbar.set_postfix({'tentativas': attempts, 'taxa_sucesso': f"{len(absences)/attempts*100:.1f}%"})
    
    if len(absences) < n_samples:
        print(f"âš ï¸  Apenas {len(absences)} pseudo-ausÃªncias foram geradas apÃ³s {attempts} tentativas")
    else:
        print(f"âœ… {len(absences)} pseudo-ausÃªncias geradas com sucesso!")
    
    return np.array(absences)

# Nova funÃ§Ã£o para realizar validaÃ§Ã£o espacial por blocos
def spatial_block_cv(coords, n_folds=5):
    """
    Divide os dados em blocos espaciais para validaÃ§Ã£o cruzada.
    Isso ajuda a evitar vazamento espacial entre treino e teste.
    """
    # Obter limites geogrÃ¡ficos
    lats, lons = zip(*coords)
    lat_min, lat_max = min(lats), max(lats)
    lon_min, lon_max = min(lons), max(lons)
    
    # Criar blocos espaciais
    lat_bins = np.linspace(lat_min, lat_max, int(np.sqrt(n_folds))+1)
    lon_bins = np.linspace(lon_min, lon_max, int(np.sqrt(n_folds))+1)
    
    # Atribuir cada ponto a um bloco
    blocks = []
    for i in range(len(coords)):
        lat, lon = coords[i]
        lat_idx = np.digitize(lat, lat_bins) - 1
        lon_idx = np.digitize(lon, lon_bins) - 1
        block_id = lat_idx * (len(lon_bins)-1) + lon_idx
        blocks.append(block_id)
    
    return np.array(blocks)

# Nova funÃ§Ã£o para avaliar importÃ¢ncia de features
def plot_feature_importance(model, feature_names, output_file="feature_importance.png"):
    """
    Gera grÃ¡fico de importÃ¢ncia das variÃ¡veis para interpretaÃ§Ã£o do modelo
    """
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.title("ImportÃ¢ncia das VariÃ¡veis BioclimÃ¡ticas")
    plt.bar(range(len(indices)), importances[indices], align="center")
    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)
    plt.tight_layout()
    plt.savefig(output_file)
    
    # TambÃ©m salva como texto
    with open("importancia_variaveis.txt", "w") as f:
        for i in indices:
            f.write(f"{feature_names[i]}: {importances[i]:.4f}\n")

def main():
    ocorrencias_csv = "ocorrencias.csv"
    rasters_atual = "clima_atual"
    rasters_futuro = "clima_futuro"

    print("ðŸ” Lendo dados de ocorrÃªncia...")
    df = pd.read_csv(ocorrencias_csv)
    df = df.dropna(subset=["longitude", "latitude"])
    coords = list(zip(df["latitude"], df["longitude"]))
    print(f"PresenÃ§as: {len(coords)}")

    print("ðŸ“š Verificando variÃ¡veis em comum...")
    arquivos_atual = {os.path.basename(f) for f in glob.glob(os.path.join(rasters_atual, "*.tif"))}
    arquivos_futuro = {os.path.basename(f) for f in glob.glob(os.path.join(rasters_futuro, "*.tif"))}
    arquivos_comuns = sorted(list(arquivos_atual & arquivos_futuro))
    if len(arquivos_comuns) < 1:
        raise ValueError("âŒ Nenhuma variÃ¡vel em comum entre clima atual e futuro.")
    print(f"âœ… VariÃ¡veis em comum: {len(arquivos_comuns)} -> {arquivos_comuns}")

    # Salva as variÃ¡veis utilizadas
    with open("variaveis_usadas.txt", "w") as f:
        for nome in arquivos_comuns:
            f.write(f"{nome}\n")

    print("ðŸ“š Empilhando clima atual...")
    clima_atual, meta_atual, nomes_variaveis = stack_rasters(rasters_atual, arquivos_comuns)
    print("âœ… Clima atual empilhado:", clima_atual.shape)

    print("ðŸ“Œ Extraindo valores ambientais para presenÃ§as...")
    presenÃ§as = extract_raster_values(coords, clima_atual, meta_atual)

    # Verificando e removendo presenÃ§as com valores NaN
    nan_mask = np.isnan(presenÃ§as).any(axis=1)
    if nan_mask.any():
        print(f"âš ï¸ Removendo {nan_mask.sum()} presenÃ§as com valores NaN")
        presenÃ§as = presenÃ§as[~nan_mask]
        presenca_coords = [coord for i, coord in enumerate(coords) if not nan_mask[i]]
    else:
        presenca_coords = coords
    
    print(f"âœ… PresenÃ§as vÃ¡lidas: {len(presenÃ§as)}")

    print("ðŸš« Gerando pseudo-ausÃªncias com distÃ¢ncia mÃ­nima de 20 km...")
    # Balanceamento entre presenÃ§as e ausÃªncias (1:1 para grandes volumes de dados)
    n_ausencias = len(presenÃ§as)  # ProporÃ§Ã£o 1:1 para maior balanceamento
    ausencias = generate_pseudo_absences(clima_atual, meta_atual, presenca_coords, n_ausencias, min_dist_km=20)

    print("ðŸ“Š Preparando dados para modelagem...")
    X = np.vstack((presenÃ§as, ausencias))
    y = np.array([1]*len(presenÃ§as) + [0]*len(ausencias))
    
    # NormalizaÃ§Ã£o dos dados (importante para variÃ¡veis em escalas diferentes)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # DivisÃ£o treino/teste considerando a estrutura espacial (se tiver coordenadas suficientes)
    print("ðŸ§® Dividindo dados para treinamento e teste...")
    if len(presenca_coords) >= 50:  # Se tiver coordenadas suficientes para blocos espaciais
        todos_coords = presenca_coords + [(0, 0)] * len(ausencias)  # Pseudo-ausÃªncias com coords dummy
        blocks = spatial_block_cv(todos_coords)
        block_ids = np.unique(blocks)
        
        # Garantir que hÃ¡ pelo menos 2 blocos diferentes para teste
        if len(block_ids) < 5:
            print("âš ï¸ Poucos blocos espaciais distintos, usando divisÃ£o aleatÃ³ria...")
            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)
        else:
            # Calcular nÃºmero mÃ­nimo de blocos para teste (pelo menos 20% dos dados)
            min_test_size = int(0.2 * len(X_scaled))
            test_blocks = []
            test_mask = np.zeros(len(blocks), dtype=bool)
            
            # Adicionar blocos ao conjunto de teste atÃ© atingir o tamanho mÃ­nimo
            while np.sum(test_mask) < min_test_size and len(test_blocks) < len(block_ids):
                # Escolher bloco aleatÃ³rio que ainda nÃ£o estÃ¡ no conjunto de teste
                remaining_blocks = [b for b in block_ids if b not in test_blocks]
                if not remaining_blocks:
                    break
                new_block = np.random.choice(remaining_blocks)
                test_blocks.append(new_block)
                
                # Atualizar mÃ¡scara de teste
                new_mask = np.isin(blocks, test_blocks)
                test_mask = new_mask
            
            # Verificar se conseguimos amostras suficientes para teste
            if np.sum(test_mask) < min_test_size * 0.5:  # Se menos da metade do desejado
                print("âš ï¸ Blocos espaciais nÃ£o forneceram amostras suficientes, usando divisÃ£o aleatÃ³ria...")
                X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)
            else:
                X_train, X_test = X_scaled[~test_mask], X_scaled[test_mask]
                y_train, y_test = y[~test_mask], y[test_mask]
                print(f"âœ… DivisÃ£o por blocos espaciais: {len(X_train)} treino, {len(X_test)} teste")
    else:
        # Fallback para divisÃ£o aleatÃ³ria estratificada tradicional
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)

    # Configurar GridSearchCV para otimizaÃ§Ã£o de hiperparÃ¢metros
    print("ðŸ” Otimizando hiperparÃ¢metros do modelo...")
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }
    
    # Usando uma versÃ£o simplificada para execuÃ§Ã£o mais rÃ¡pida
    param_grid_simples = {
        'n_estimators': [100],
        'max_depth': [None, 15],
        'min_samples_split': [5],
        'min_samples_leaf': [2],
        'max_features': ['sqrt']
    }
    
    # Escolha qual grid usar com base no tamanho do conjunto de dados
    grid = param_grid_simples if len(X_train) > 1000 else param_grid
    
    rf = RandomForestClassifier(random_state=42, class_weight='balanced')
    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=grid,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"âœ… Melhores parÃ¢metros encontrados: {grid_search.best_params_}")
    model = grid_search.best_estimator_
    
    # AvaliaÃ§Ã£o no conjunto de teste
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = model.predict(X_test)
    test_auc = roc_auc_score(y_test, y_pred_proba)
    
    print(f"âœ… Desempenho no conjunto de teste:")
    print(f"   AUC: {test_auc:.3f}")
    print("\nMatriz de confusÃ£o:")
    print(confusion_matrix(y_test, y_pred))
    print("\nRelatÃ³rio de classificaÃ§Ã£o:")
    print(classification_report(y_test, y_pred))
    
    # Calcular importÃ¢ncia das variÃ¡veis
    print("ðŸ“Š Calculando importÃ¢ncia das variÃ¡veis...")
    plot_feature_importance(model, nomes_variaveis)
    
    # Salvar modelo e scaler para uso posterior
    joblib.dump(model, "modelo_random_forest.pkl")
    joblib.dump(scaler, "scaler.pkl")
    
    # Projetar para clima atual
    print("ðŸ—ºï¸ Projetando distribuiÃ§Ã£o atual...")
    # Redimensionar e normalizar os dados antes da prediÃ§Ã£o
    clima_atual_reshaped = clima_atual.reshape(-1, clima_atual.shape[2])
    # Criar mÃ¡scara para valores vÃ¡lidos (nÃ£o-NaN)
    valid_mask = ~np.isnan(clima_atual_reshaped).any(axis=1)
    valid_indices = np.where(valid_mask)[0]
    
    # Inicializar mapa de prediÃ§Ã£o com NaN
    pred_map = np.full(clima_atual_reshaped.shape[0], np.nan)
    
    # Aplicar scaler e prever apenas para pixels vÃ¡lidos
    if len(valid_indices) > 0:
        valid_data = clima_atual_reshaped[valid_indices]
        valid_data_scaled = scaler.transform(valid_data)
        valid_preds = model.predict_proba(valid_data_scaled)[:, 1]
        pred_map[valid_indices] = valid_preds
    
    # Redimensionar para formato original
    pred_map = pred_map.reshape(clima_atual.shape[0], clima_atual.shape[1])
    
    # Salvar mapa
    with rasterio.open("mapa_predito_atual_brasil.tif", "w", **meta_atual) as dst:
        dst.write(pred_map, 1)
    
    # Processar clima futuro
    print("ðŸ“¦ Empilhando clima futuro...")
    clima_futuro, meta_futuro, _ = stack_rasters(rasters_futuro, arquivos_comuns)
    
    print("ðŸ—ºï¸ Projetando distribuiÃ§Ã£o futura...")
    # Mesmo processo de prediÃ§Ã£o para clima futuro
    clima_futuro_reshaped = clima_futuro.reshape(-1, clima_futuro.shape[2])
    valid_mask_futuro = ~np.isnan(clima_futuro_reshaped).any(axis=1)
    valid_indices_futuro = np.where(valid_mask_futuro)[0]
    
    pred_futuro = np.full(clima_futuro_reshaped.shape[0], np.nan)
    
    if len(valid_indices_futuro) > 0:
        valid_data_futuro = clima_futuro_reshaped[valid_indices_futuro]
        valid_data_futuro_scaled = scaler.transform(valid_data_futuro)
        valid_preds_futuro = model.predict_proba(valid_data_futuro_scaled)[:, 1]
        pred_futuro[valid_indices_futuro] = valid_preds_futuro
    
    pred_futuro = pred_futuro.reshape(clima_futuro.shape[0], clima_futuro.shape[1])
    
    with rasterio.open("mapa_predito_futuro_brasil.tif", "w", **meta_futuro) as dst:
        dst.write(pred_futuro, 1)
    
    # Calcular mudanÃ§a entre atual e futuro
    if pred_map.shape == pred_futuro.shape:
        print("ðŸ“Š Calculando mapa de mudanÃ§a (futuro - atual)...")
        mapa_mudanca = pred_futuro - pred_map
        with rasterio.open("mapa_mudanca_brasil.tif", "w", **meta_atual) as dst:
            dst.write(mapa_mudanca, 1)
    
    print("âœ… Modelagem concluÃ­da!")

if __name__ == "__main__":
    main()
