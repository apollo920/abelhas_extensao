{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb194a5",
   "metadata": {},
   "source": [
    "## Algoritmo Random Forest\n",
    "- Previs√£o do posicionamento da Abelha Trigona Spinipes no per√≠odo de 2021-2040, 2041-2060, 2061-80 e 2081-2100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5ccfb",
   "metadata": {},
   "source": [
    "### Configura√ß√£o de Ambiente \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8679c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias necess√°rias\n",
    "# pip install pandas geopandas rasterio scikit-learn matplotlib streamlit folium streamlit-folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72de8d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente configurado e caminhos definidos automaticamente com pathlib!\n",
      "Pasta raiz do projeto encontrada: c:\\icev\\extensao\\abelhas_extensao\n",
      "Caminho do arquivo de ocorr√™ncias: c:\\icev\\extensao\\abelhas_extensao\\data\\ocorrencias.csv\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas (adicionando pathlib)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import geometry_mask\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path  # <<< IMPORTAMOS A BIBLIOTECA PATHLIB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Ignorar avisos para uma sa√≠da mais limpa\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINI√á√ÉO AUTOM√ÅTICA DOS CAMINHOS ---\n",
    "# Esta linha m√°gica encontra o caminho da pasta do projeto automaticamente!\n",
    "# Path.cwd() pega o diret√≥rio atual (.../abelhas_extensao/notebooks)\n",
    "# .parent sobe um n√≠vel (.../abelhas_extensao)\n",
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "# Definir os caminhos completos usando o operador / do pathlib\n",
    "OCORRENCIAS_PATH = BASE_DIR / 'data' / 'ocorrencias.csv'\n",
    "CLIMA_ATUAL_PATH = BASE_DIR / 'data' / 'clima_atual'\n",
    "BRASIL_SHAPE_PATH = BASE_DIR / 'data' / 'BR_UF_2024'\n",
    "PASTA_PREVISOES = BASE_DIR / 'data' / 'previsoes_futuras'\n",
    "\n",
    "# Criar a pasta para salvar as previs√µes, se ela n√£o existir\n",
    "PASTA_PREVISOES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Ambiente configurado e caminhos definidos automaticamente com pathlib!\")\n",
    "print(f\"Pasta raiz do projeto encontrada: {BASE_DIR}\")\n",
    "print(f\"Caminho do arquivo de ocorr√™ncias: {OCORRENCIAS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674a6df",
   "metadata": {},
   "source": [
    "### Tratamento e Carregamento dos Dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd89c36",
   "metadata": {},
   "source": [
    "#### Carregar Dados de Ocorr√™ncia e Mapa do Brasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8fc570fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentando carregar o arquivo 'ocorrencias.csv'...\n",
      "‚úÖ Arquivo carregado com sucesso!\n",
      "Total de pontos de ocorr√™ncia carregados: 15244\n",
      "   decimalLatitude  decimalLongitude\n",
      "0          -9.2963          -75.9972\n",
      "1         -23.4671          -56.4886\n",
      "2         -23.4671          -56.4886\n",
      "3         -23.4671          -56.4886\n",
      "4         -23.4671          -56.4886\n",
      "\n",
      "Mapa do Brasil e pontos de ocorr√™ncia carregados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados de ocorr√™ncia de forma robusta (VERS√ÉO FINAL CORRIGIDA)\n",
    "try:\n",
    "    print(\"Tentando carregar o arquivo 'ocorrencias.csv'...\")\n",
    "    ocorrencias_df_full = pd.read_csv(\n",
    "        OCORRENCIAS_PATH,\n",
    "        sep='\\t',                     # <<< A MUDAN√áA CHAVE! Diz ao pandas para usar tabula√ß√£o como separador.\n",
    "        comment='#',\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False,\n",
    "        encoding='utf-8-sig'\n",
    "    )\n",
    "    \n",
    "    # Agora, selecionamos apenas as colunas que nos interessamos e removemos valores nulos\n",
    "    ocorrencias_df = ocorrencias_df_full[['decimalLatitude', 'decimalLongitude']].dropna()\n",
    "    \n",
    "    print(\"‚úÖ Arquivo carregado com sucesso!\")\n",
    "    print(f\"Total de pontos de ocorr√™ncia carregados: {len(ocorrencias_df)}\")\n",
    "    print(ocorrencias_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Erro: Arquivo n√£o encontrado em '{OCORRENCIAS_PATH}'. Verifique se o caminho est√° correto.\")\n",
    "except KeyError:\n",
    "    print(\"‚ùå Erro: As colunas 'decimalLatitude' ou 'decimalLongitude' n√£o foram encontradas no arquivo.\")\n",
    "    print(\"Verifique os nomes das colunas no cabe√ßalho do seu CSV.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ocorreu um erro inesperado ao carregar o arquivo: {e}\")\n",
    "\n",
    "# Converter o DataFrame para um GeoDataFrame\n",
    "gdf_ocorrencias = gpd.GeoDataFrame(\n",
    "    ocorrencias_df,\n",
    "    geometry=gpd.points_from_xy(ocorrencias_df.decimalLongitude, ocorrencias_df.decimalLatitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Carregar o mapa do Brasil (shapefile)\n",
    "shapefile_brasil = glob.glob(os.path.join(BRASIL_SHAPE_PATH, \"*.shp\"))[0]\n",
    "brasil_gdf = gpd.read_file(shapefile_brasil)\n",
    "\n",
    "# Unir todos os estados em um √∫nico pol√≠gono do Brasil\n",
    "brasil_poligono = brasil_gdf.unary_union\n",
    "\n",
    "print(\"\\nMapa do Brasil e pontos de ocorr√™ncia carregados com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484058bf",
   "metadata": {},
   "source": [
    "#### Carregar e empilhar Dados Clim√°ticos Atuais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf3c8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasters clim√°ticos atuais empilhados em: c:\\icev\\extensao\\abelhas_extensao\\data\\clima_atual\\clima_atual_stack.tif\n"
     ]
    }
   ],
   "source": [
    "# Listar todos os arquivos .tif de clima atual, em ordem alfab√©tica\n",
    "clima_files = sorted(glob.glob(os.path.join(CLIMA_ATUAL_PATH, \"*.tif\")))\n",
    "\n",
    "# Abrir o primeiro arquivo para obter metadados\n",
    "with rasterio.open(clima_files[0]) as src:\n",
    "    meta = src.meta\n",
    "\n",
    "# Atualizar os metadados para o novo raster empilhado (agora com 19 bandas)\n",
    "meta.update(count=len(clima_files))\n",
    "\n",
    "# Criar o caminho para o arquivo empilhado\n",
    "stack_path = os.path.join(CLIMA_ATUAL_PATH, \"clima_atual_stack.tif\")\n",
    "\n",
    "# Empilhar os rasters em um √∫nico arquivo\n",
    "with rasterio.open(stack_path, 'w', **meta) as dst:\n",
    "    for i, file in enumerate(clima_files, 1):\n",
    "        with rasterio.open(file) as src:\n",
    "            dst.write(src.read(1), i)\n",
    "\n",
    "print(f\"Rasters clim√°ticos atuais empilhados em: {stack_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd1482",
   "metadata": {},
   "source": [
    "#### Gerar Dados de Pseudo-Aus√™ncia\n",
    "- Geramos pontos de pseudo-aus√™ncia em √°reas aleat√≥rias, mas longe dos pontos de presen√ßa, para treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50c3d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerados 30488 pontos de pseudo-aus√™ncia.\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "# N√∫mero de pontos de pseudo-aus√™ncia\n",
    "num_pseudo_ausencias = len(gdf_ocorrencias) * 2\n",
    "\n",
    "# Criar uma \"zona de exclus√£o\" ao redor dos pontos de presen√ßa\n",
    "buffer_presenca = gdf_ocorrencias.geometry.buffer(0.5).unary_union\n",
    "\n",
    "# Gerar pontos aleat√≥rios dentro do pol√≠gono do Brasil\n",
    "pseudo_ausencias_points = []\n",
    "while len(pseudo_ausencias_points) < num_pseudo_ausencias:\n",
    "    minx, miny, maxx, maxy = brasil_poligono.bounds\n",
    "    random_point = Point(np.random.uniform(minx, maxx), np.random.uniform(miny, maxy))\n",
    "    if brasil_poligono.contains(random_point) and not buffer_presenca.contains(random_point):\n",
    "        pseudo_ausencias_points.append(random_point)\n",
    "\n",
    "# Criar um GeoDataFrame para as pseudo-aus√™ncias\n",
    "gdf_pseudo_ausencias = gpd.GeoDataFrame(\n",
    "    geometry=pseudo_ausencias_points,\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "print(f\"Gerados {len(gdf_pseudo_ausencias)} pontos de pseudo-aus√™ncia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fc8d4",
   "metadata": {},
   "source": [
    "#### Criar um Conjunto de Dados de Treinamento Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b98a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de dados de treinamento criado:\n",
      "         wc2        wc2        wc2     wc2    wc2   wc2        wc2     wc2  \\\n",
      "0  24.271093  24.590834  24.027500  2825.0  363.0  98.0  42.891392  1032.0   \n",
      "1  23.157845  26.684959  19.153584  1513.0  184.0  57.0  34.639950   520.0   \n",
      "2  23.157845  26.684959  19.153584  1513.0  184.0  57.0  34.639950   520.0   \n",
      "3  23.157845  26.684959  19.153584  1513.0  184.0  57.0  34.639950   520.0   \n",
      "4  23.157845  26.684959  19.153584  1513.0  184.0  57.0  34.639950   520.0   \n",
      "\n",
      "     wc2    wc2     wc2        wc2        wc2         wc2        wc2  \\\n",
      "0  335.0  719.0  1020.0  11.812813  87.883148   28.844713  30.877750   \n",
      "1  198.0  474.0   198.0  11.594021  57.734829  317.950592  32.429501   \n",
      "2  198.0  474.0   198.0  11.594021  57.734829  317.950592  32.429501   \n",
      "3  198.0  474.0   198.0  11.594021  57.734829  317.950592  32.429501   \n",
      "4  198.0  474.0   198.0  11.594021  57.734829  317.950592  32.429501   \n",
      "\n",
      "         wc2        wc2        wc2        wc2  presenca  \n",
      "0  17.436251  13.441500  24.330458  24.035416         1  \n",
      "1  12.348000  20.081501  25.261999  19.153584         1  \n",
      "2  12.348000  20.081501  25.261999  19.153584         1  \n",
      "3  12.348000  20.081501  25.261999  19.153584         1  \n",
      "4  12.348000  20.081501  25.261999  19.153584         1  \n",
      "\n",
      "Shape de X: (45732, 19), Shape de y: (45732,)\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para extrair valores do raster para um GeoDataFrame\n",
    "def extract_raster_values(gdf, raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        coords = [(x, y) for x, y in zip(gdf.geometry.x, gdf.geometry.y)]\n",
    "        values = [val for val in src.sample(coords)]\n",
    "    return np.array(values)\n",
    "\n",
    "# Extrair valores para presen√ßas e pseudo-aus√™ncias\n",
    "valores_presenca = extract_raster_values(gdf_ocorrencias, stack_path)\n",
    "valores_ausencia = extract_raster_values(gdf_pseudo_ausencias, stack_path)\n",
    "\n",
    "# Criar o dataset final\n",
    "X = np.vstack((valores_presenca, valores_ausencia))\n",
    "y = np.array([1] * len(valores_presenca) + [0] * len(valores_ausencia))\n",
    "\n",
    "# Nomes das features (bio1 a bio19)\n",
    "feature_names = [os.path.basename(f).split('.')[0] for f in sorted(clima_files)]\n",
    "\n",
    "# Criar um DataFrame para visualiza√ß√£o\n",
    "df_treinamento = pd.DataFrame(X, columns=feature_names)\n",
    "df_treinamento['presenca'] = y\n",
    "\n",
    "print(\"Conjunto de dados de treinamento criado:\")\n",
    "print(df_treinamento.head())\n",
    "print(f\"\\nShape de X: {X.shape}, Shape de y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ab8c9",
   "metadata": {},
   "source": [
    "### Treinamento do Modelo Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ab8e5",
   "metadata": {},
   "source": [
    "#### Dividir os Dados e Treinar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4743e42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o modelo Random Forest...\n",
      "Treinamento conclu√≠do!\n"
     ]
    }
   ],
   "source": [
    "# Dividir os dados em conjuntos de treino (80%) e teste (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Inicializar o modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "print(\"Treinando o modelo Random Forest...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Treinamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45305a1b",
   "metadata": {},
   "source": [
    "#### Avaliar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "edb99024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia do modelo no conjunto de teste: 1.00\n",
      "\n",
      "Relat√≥rio de Classifica√ß√£o:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6098\n",
      "           1       1.00      0.99      0.99      3049\n",
      "\n",
      "    accuracy                           1.00      9147\n",
      "   macro avg       1.00      0.99      1.00      9147\n",
      "weighted avg       1.00      1.00      1.00      9147\n",
      "\n",
      "\n",
      "Import√¢ncia das Vari√°veis Clim√°ticas:\n",
      "   variavel  importancia\n",
      "14      wc2     0.164282\n",
      "13      wc2     0.130456\n",
      "2       wc2     0.095245\n",
      "0       wc2     0.087258\n",
      "7       wc2     0.067913\n",
      "11      wc2     0.059742\n",
      "17      wc2     0.050944\n",
      "12      wc2     0.047665\n",
      "16      wc2     0.036975\n",
      "1       wc2     0.032330\n",
      "3       wc2     0.032079\n",
      "4       wc2     0.027911\n",
      "15      wc2     0.027134\n",
      "9       wc2     0.026399\n",
      "18      wc2     0.026199\n",
      "6       wc2     0.025042\n",
      "10      wc2     0.024147\n",
      "5       wc2     0.020117\n",
      "8       wc2     0.018162\n"
     ]
    }
   ],
   "source": [
    "# Fazer previs√µes no conjunto de teste\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Avaliar a acur√°cia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acur√°cia do modelo no conjunto de teste: {accuracy:.2f}\")\n",
    "\n",
    "# Mostrar um relat√≥rio de classifica√ß√£o detalhado\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Analisar a import√¢ncia de cada vari√°vel clim√°tica\n",
    "importancias = pd.DataFrame({\n",
    "    'variavel': feature_names,\n",
    "    'importancia': rf_model.feature_importances_\n",
    "}).sort_values('importancia', ascending=False)\n",
    "\n",
    "print(\"\\nImport√¢ncia das Vari√°veis Clim√°ticas:\")\n",
    "print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fe838",
   "metadata": {},
   "source": [
    "### Previs√£o para cen√°rios futuros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bec70a",
   "metadata": {},
   "source": [
    "#### Definir a Fun√ß√£o de Previs√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "80d409ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prever_cenario(cenario_folder_path, modelo, output_path):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para prever a adequabilidade de habitat para um cen√°rio clim√°tico futuro.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessando cen√°rio em: {cenario_folder_path}\")\n",
    "    \n",
    "    cenario_files = sorted(glob.glob(os.path.join(cenario_folder_path, \"*.tif\")))\n",
    "    \n",
    "    with rasterio.open(cenario_files[0]) as src:\n",
    "        profile = src.profile\n",
    "        \n",
    "    raster_data = np.stack([rasterio.open(f).read(1) for f in cenario_files])\n",
    "    height, width = raster_data.shape[1], raster_data.shape[2]\n",
    "    raster_data_reshaped = raster_data.reshape((len(cenario_files), -1)).T\n",
    "    \n",
    "    # Tratar valores NoData\n",
    "    nodata_val = -9999.0\n",
    "    raster_data_reshaped[raster_data_reshaped == nodata_val] = np.nan\n",
    "    col_mean = np.nanmean(raster_data_reshaped, axis=0)\n",
    "    inds = np.where(np.isnan(raster_data_reshaped))\n",
    "    raster_data_reshaped[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "    print(\"Realizando a previs√£o...\")\n",
    "    previsao = modelo.predict_proba(raster_data_reshaped)[:, 1]\n",
    "    previsao_mapa = previsao.reshape((height, width))\n",
    "    \n",
    "    profile.update(dtype=rasterio.float32, count=1, compress='lzw')\n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(previsao_mapa.astype(rasterio.float32), 1)\n",
    "        \n",
    "    print(f\"Mapa de adequabilidade salvo em: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fa08d",
   "metadata": {},
   "source": [
    "#### Executar as Previs√µes para todos os Per√≠odos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d1ec1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Diagn√≥stico das Pastas de Clima Futuro ---\n",
      "\n",
      "Verificando a pasta: c:\\icev\\extensao\\abelhas_extensao\\data\\clima_futuro\\wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2021-2040\n",
      "‚ùå ERRO: A pasta 'wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2021-2040' n√£o existe!\n",
      "\n",
      "Verificando a pasta: c:\\icev\\extensao\\abelhas_extensao\\data\\clima_futuro\\wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2041-2060\n",
      "‚ùå ERRO: A pasta 'wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2041-2060' n√£o existe!\n",
      "\n",
      "Verificando a pasta: c:\\icev\\extensao\\abelhas_extensao\\data\\clima_futuro\\wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2061-2080\n",
      "‚ùå ERRO: A pasta 'wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2061-2080' n√£o existe!\n",
      "\n",
      "Verificando a pasta: c:\\icev\\extensao\\abelhas_extensao\\data\\clima_futuro\\wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2081-2100\n",
      "‚ùå ERRO: A pasta 'wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2081-2100' n√£o existe!\n",
      "\n",
      "--- SOLU√á√ÉO ---\n",
      "A pasta 'wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2081-2100' est√° com problemas.\n",
      "Para corrigir:\n",
      "1. Encontre o arquivo .zip original do WorldClim correspondente a 'wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2081-2100'.\n",
      "2. Apague a pasta vazia (ou com problemas) de 'data/clima_futuro/'.\n",
      "3. Extraia novamente o conte√∫do do .zip para a pasta 'data/clima_futuro/'.\n"
     ]
    }
   ],
   "source": [
    "# --- C√âLULA DE DIAGN√ìSTICO DAS PASTAS DE CLIMA FUTURO ---\n",
    "\n",
    "print(\"--- Diagn√≥stico das Pastas de Clima Futuro ---\")\n",
    "\n",
    "# Lista dos per√≠odos futuros com os nomes EXATOS das suas pastas\n",
    "periodos_futuros = [\n",
    "    \"wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2021-2040\",\n",
    "    \"wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2041-2060\",\n",
    "    \"wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2061-2080\",\n",
    "    \"wc2.1_10m_bioc_BCC-CSM2-MR_ssp245_2081-2100\"\n",
    "]\n",
    "\n",
    "clima_futuro_base_path = BASE_DIR / 'data' / 'clima_futuro'\n",
    "\n",
    "pasta_com_problema = None\n",
    "\n",
    "for periodo in periodos_futuros:\n",
    "    pasta_cenario = clima_futuro_base_path / periodo\n",
    "    print(f\"\\nVerificando a pasta: {pasta_cenario}\")\n",
    "    \n",
    "    if not pasta_cenario.exists():\n",
    "        print(f\"‚ùå ERRO: A pasta '{periodo}' n√£o existe!\")\n",
    "        pasta_com_problema = periodo\n",
    "        continue\n",
    "    \n",
    "    # Listar os arquivos .tif na pasta\n",
    "    tif_files = sorted(glob.glob(os.path.join(pasta_cenario, \"*.tif\")))\n",
    "    \n",
    "    if not tif_files:\n",
    "        print(f\"‚ùå ERRO: A pasta '{periodo}' est√° vazia ou n√£o cont√©m arquivos .tif!\")\n",
    "        pasta_com_problema = periodo\n",
    "    else:\n",
    "        print(f\"‚úÖ Encontrados {len(tif_files)} arquivos .tif na pasta '{periodo}'.\")\n",
    "        # Opcional: mostrar os 3 primeiros arquivos\n",
    "        print(f\"   Exemplos: {os.path.basename(tif_files[0])}, {os.path.basename(tif_files[1])}, {os.path.basename(tif_files[2])}\")\n",
    "\n",
    "if pasta_com_problema:\n",
    "    print(\"\\n--- SOLU√á√ÉO ---\")\n",
    "    print(f\"A pasta '{pasta_com_problema}' est√° com problemas.\")\n",
    "    print(\"Para corrigir:\")\n",
    "    print(f\"1. Encontre o arquivo .zip original do WorldClim correspondente a '{pasta_com_problema}'.\")\n",
    "    print(\"2. Apague a pasta vazia (ou com problemas) de 'data/clima_futuro/'.\")\n",
    "    print(\"3. Extraia novamente o conte√∫do do .zip para a pasta 'data/clima_futuro/'.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todas as pastas de cen√°rio futuro parecem estar OK!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd251840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Nenhum mapa mascarado encontrado!\n",
      "Execute primeiro:\n",
      "1. python gerar_mapas_ajustados.py\n",
      "2. python mascarar_brasil.py\n",
      "üìÇ Mapas detectados:\n",
      "\n",
      "üé® Escolha o tipo de escala para visualiza√ß√£o:\n",
      "1. Escala padronizada (0-1 para probabilidades, -0.8 a +0.8 para mudan√ßas)\n",
      "2. Escala din√¢mica (baseada nos valores dos dados)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def detectar_mapas_brasil():\n",
    "    \"\"\"Detecta automaticamente os mapas mascarados do Brasil\"\"\"\n",
    "    mapas_encontrados = {}\n",
    "    \n",
    "    # Procurar mapas de distribui√ß√£o atual\n",
    "    atual_files = glob.glob(\"mapa_predito_atual_*_brasil.tif\")\n",
    "    if atual_files:\n",
    "        mapas_encontrados[\"atual\"] = atual_files[0]\n",
    "    \n",
    "    # Procurar mapas de distribui√ß√£o futura\n",
    "    futuro_files = glob.glob(\"mapa_predito_futuro*_brasil.tif\")\n",
    "    if futuro_files:\n",
    "        mapas_encontrados[\"futuro\"] = futuro_files[0]\n",
    "    \n",
    "    # Procurar mapas de mudan√ßa\n",
    "    mudanca_files = glob.glob(\"mapa_mudanca_*_brasil.tif\")\n",
    "    if mudanca_files:\n",
    "        mapas_encontrados[\"mudanca\"] = mudanca_files[0]\n",
    "    \n",
    "    return mapas_encontrados\n",
    "\n",
    "def visualizar_mapa(tif_path, titulo, salvar=True, escala_padronizada=True):\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        data = src.read(1).astype(float)\n",
    "\n",
    "        # Trata valores nodata\n",
    "        if src.nodata is not None:\n",
    "            data[data == src.nodata] = np.nan\n",
    "\n",
    "        # Obt√©m valores m√≠nimo e m√°ximo reais para estat√≠sticas\n",
    "        valid_data = data[~np.isnan(data)]\n",
    "        if len(valid_data) == 0:\n",
    "            print(f\"‚ùå Nenhum dado v√°lido encontrado em {titulo}\")\n",
    "            return\n",
    "            \n",
    "        v_min_real = np.nanmin(valid_data)\n",
    "        v_max_real = np.nanmax(valid_data)\n",
    "        \n",
    "        # Calcula estat√≠sticas b√°sicas\n",
    "        mean = np.nanmean(valid_data)\n",
    "        std = np.nanstd(valid_data)\n",
    "        \n",
    "        print(f\"üìä Estat√≠sticas para {titulo}:\")\n",
    "        print(f\"   - Min: {v_min_real:.4f}, Max: {v_max_real:.4f}, M√©dia: {mean:.4f}, Desvio: {std:.4f}\")\n",
    "        \n",
    "        # Define escala de visualiza√ß√£o\n",
    "        if escala_padronizada:\n",
    "            if \"mudan√ßa\" in titulo.lower() or \"mudanca\" in titulo.lower():\n",
    "                # Para mapas de mudan√ßa: escala sim√©trica baseada no m√°ximo absoluto global\n",
    "                abs_max = max(abs(v_min_real), abs(v_max_real))\n",
    "                # Usar uma escala padr√£o de -0.8 a +0.8 para mudan√ßas\n",
    "                v_min, v_max = -0.8, 0.8\n",
    "                cmap = plt.cm.RdBu_r  # Vermelho = perda, Azul = ganho\n",
    "                label = 'Mudan√ßa na Probabilidade'\n",
    "                print(f\"   üìè Escala padronizada para mudan√ßa: {v_min} a {v_max}\")\n",
    "            else:\n",
    "                # Para mapas de probabilidade: escala padronizada 0-1\n",
    "                v_min, v_max = 0.0, 1.0\n",
    "                cmap = plt.cm.viridis\n",
    "                label = 'Probabilidade de Adequabilidade'\n",
    "                print(f\"   üìè Escala padronizada para probabilidade: {v_min} a {v_max}\")\n",
    "        else:\n",
    "            # Escala din√¢mica baseada nos dados\n",
    "            v_min, v_max = v_min_real, v_max_real\n",
    "            if \"mudan√ßa\" in titulo.lower() or \"mudanca\" in titulo.lower():\n",
    "                cmap = plt.cm.RdBu_r\n",
    "                label = 'Mudan√ßa na Probabilidade'\n",
    "            else:\n",
    "                cmap = plt.cm.viridis\n",
    "                label = 'Probabilidade de Adequabilidade'\n",
    "            print(f\"   üìè Escala din√¢mica: {v_min:.4f} a {v_max:.4f}\")\n",
    "        \n",
    "        # Configura visualiza√ß√£o\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Cria o mapa\n",
    "        im = plt.imshow(data, cmap=cmap, vmin=v_min, vmax=v_max)\n",
    "        \n",
    "        # Adiciona t√≠tulo e legenda\n",
    "        plt.title(titulo, fontsize=16, pad=20)\n",
    "        plt.axis('off')\n",
    "        cbar = plt.colorbar(im, label=label, shrink=0.7)\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "        \n",
    "        # Adiciona informa√ß√µes sobre valores reais e escala\n",
    "        info_text = f\"Valores reais: {v_min_real:.4f} a {v_max_real:.4f}\\nM√©dia: {mean:.4f} ¬± {std:.4f}\"\n",
    "        if escala_padronizada:\n",
    "            info_text += f\"\\nEscala padronizada: {v_min} a {v_max}\"\n",
    "        \n",
    "        plt.annotate(info_text, \n",
    "                     xy=(0.02, 0.02), xycoords='figure fraction', \n",
    "                     fontsize=11, color='black', backgroundcolor='white',\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salva o mapa em alta resolu√ß√£o para o trabalho acad√™mico\n",
    "        if salvar:\n",
    "            nome_arquivo = f\"{titulo.replace(' ', '_').replace('(', '').replace(')', '')}.png\"\n",
    "            plt.savefig(nome_arquivo, dpi=300, bbox_inches='tight')\n",
    "            print(f\"üíæ Mapa salvo como: {nome_arquivo}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "def visualizar_mapa_diferenca(atual_path, futuro_path, escala_padronizada=True):\n",
    "    \"\"\"Cria um mapa de diferen√ßa entre distribui√ß√£o futura e atual\"\"\"\n",
    "    \n",
    "    if not os.path.exists(atual_path) or not os.path.exists(futuro_path):\n",
    "        print(\"‚ùå Arquivos de mapa n√£o encontrados para calcular diferen√ßa\")\n",
    "        return\n",
    "    \n",
    "    with rasterio.open(atual_path) as src_atual, rasterio.open(futuro_path) as src_futuro:\n",
    "        data_atual = src_atual.read(1).astype(float)\n",
    "        data_futuro = src_futuro.read(1).astype(float)\n",
    "        \n",
    "        # Trata valores nodata\n",
    "        if src_atual.nodata is not None:\n",
    "            data_atual[data_atual == src_atual.nodata] = np.nan\n",
    "        if src_futuro.nodata is not None:\n",
    "            data_futuro[data_futuro == src_futuro.nodata] = np.nan\n",
    "        \n",
    "        # Calcula diferen√ßa\n",
    "        data_diff = data_futuro - data_atual\n",
    "        \n",
    "        # Configurar visualiza√ß√£o\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Paleta divergente para mudan√ßas (vermelho = perda, azul = ganho)\n",
    "        cmap = plt.cm.RdBu_r\n",
    "        \n",
    "        # Determinar limite para visualiza√ß√£o\n",
    "        valid_diff = data_diff[~np.isnan(data_diff)]\n",
    "        if len(valid_diff) == 0:\n",
    "            print(\"‚ùå Nenhum dado v√°lido para calcular diferen√ßa\")\n",
    "            return\n",
    "        \n",
    "        v_min_real = np.nanmin(data_diff)\n",
    "        v_max_real = np.nanmax(data_diff)\n",
    "        \n",
    "        if escala_padronizada:\n",
    "            # Escala padronizada para mudan√ßas\n",
    "            v_min, v_max = -0.8, 0.8\n",
    "        else:\n",
    "            # Escala din√¢mica sim√©trica\n",
    "            abs_max = max(abs(v_min_real), abs(v_max_real))\n",
    "            v_min, v_max = -abs_max, abs_max\n",
    "        \n",
    "        im = plt.imshow(data_diff, cmap=cmap, vmin=v_min, vmax=v_max)\n",
    "        \n",
    "        plt.title(\"Mudan√ßa na Distribui√ß√£o (Futuro - Atual)\", fontsize=16, pad=20)\n",
    "        plt.axis('off')\n",
    "        cbar = plt.colorbar(im, label='Mudan√ßa na Probabilidade', shrink=0.7)\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "        \n",
    "        # Calcular estat√≠sticas da diferen√ßa\n",
    "        mean_diff = np.nanmean(data_diff)\n",
    "        perc_loss = np.sum(data_diff < -0.1) / np.sum(~np.isnan(data_diff)) * 100\n",
    "        perc_gain = np.sum(data_diff > 0.1) / np.sum(~np.isnan(data_diff)) * 100\n",
    "        \n",
    "        # Adiciona informa√ß√µes sobre valores\n",
    "        info_text = (f\"Valores reais: {v_min_real:.4f} a {v_max_real:.4f}\\n\"\n",
    "                     f\"M√©dia da mudan√ßa: {mean_diff:.4f}\\n\"\n",
    "                     f\"√Årea com perda (< -0.1): {perc_loss:.1f}%\\n\"\n",
    "                     f\"√Årea com ganho (> 0.1): {perc_gain:.1f}%\")\n",
    "        \n",
    "        if escala_padronizada:\n",
    "            info_text += f\"\\nEscala padronizada: {v_min} a {v_max}\"\n",
    "        \n",
    "        plt.annotate(info_text, xy=(0.02, 0.02), xycoords='figure fraction', \n",
    "                     fontsize=11, color='black', backgroundcolor='white',\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salva o mapa\n",
    "        plt.savefig(\"Mudanca_na_Distribuicao.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Mapa de mudan√ßa salvo como: Mudanca_na_Distribuicao.png\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Detectar mapas automaticamente\n",
    "    mapas = detectar_mapas_brasil()\n",
    "    \n",
    "    if not mapas:\n",
    "        print(\"‚ùå Nenhum mapa mascarado encontrado!\")\n",
    "        print(\"Execute primeiro:\")\n",
    "        print(\"1. python gerar_mapas_ajustados.py\")\n",
    "        print(\"2. python mascarar_brasil.py\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"üìÇ Mapas detectados:\")\n",
    "    for tipo, arquivo in mapas.items():\n",
    "        print(f\"  - {tipo}: {arquivo}\")\n",
    "    \n",
    "    # Perguntar sobre padroniza√ß√£o\n",
    "    print(\"\\nüé® Escolha o tipo de escala para visualiza√ß√£o:\")\n",
    "    print(\"1. Escala padronizada (0-1 para probabilidades, -0.8 a +0.8 para mudan√ßas)\")\n",
    "    print(\"2. Escala din√¢mica (baseada nos valores dos dados)\")\n",
    "    opcao_escala = input(\"Op√ß√£o (1 ou 2) [padr√£o=1]: \").strip() or \"1\"\n",
    "    \n",
    "    escala_padronizada = opcao_escala == \"1\"\n",
    "    \n",
    "    if escala_padronizada:\n",
    "        print(\"‚úÖ Usando escalas padronizadas para compara√ß√£o cient√≠fica\")\n",
    "    else:\n",
    "        print(\"‚úÖ Usando escalas din√¢micas para visualiza√ß√£o detalhada\")\n",
    "    \n",
    "    # Visualizar mapas de distribui√ß√£o\n",
    "    if \"atual\" in mapas:\n",
    "        print(f\"\\nüìç Exibindo: Distribui√ß√£o Atual\")\n",
    "        # Extrair sufixo do arquivo para identificar o cen√°rio\n",
    "        nome_arquivo = os.path.basename(mapas[\"atual\"])\n",
    "        if \"futuro2\" in nome_arquivo:\n",
    "            sufixo = \" (cen√°rio futuro2)\"\n",
    "        elif \"futuro3\" in nome_arquivo:\n",
    "            sufixo = \" (cen√°rio futuro3)\"\n",
    "        else:\n",
    "            sufixo = \"\"\n",
    "        visualizar_mapa(mapas[\"atual\"], f\"Distribui√ß√£o Atual{sufixo}\", escala_padronizada=escala_padronizada)\n",
    "    \n",
    "    if \"futuro\" in mapas:\n",
    "        print(f\"\\nüìç Exibindo: Distribui√ß√£o Futura\")\n",
    "        # Extrair sufixo do arquivo para identificar o cen√°rio\n",
    "        nome_arquivo = os.path.basename(mapas[\"futuro\"])\n",
    "        if \"futuro2\" in nome_arquivo:\n",
    "            sufixo = \" (cen√°rio futuro2)\"\n",
    "        elif \"futuro3\" in nome_arquivo:\n",
    "            sufixo = \" (cen√°rio futuro3)\"\n",
    "        else:\n",
    "            sufixo = \"\"\n",
    "        visualizar_mapa(mapas[\"futuro\"], f\"Distribui√ß√£o Futura{sufixo}\", escala_padronizada=escala_padronizada)\n",
    "    \n",
    "    # Visualizar mapa de mudan√ßa se dispon√≠vel\n",
    "    if \"mudanca\" in mapas:\n",
    "        print(f\"\\nüìç Exibindo: Mapa de Mudan√ßa\")\n",
    "        nome_arquivo = os.path.basename(mapas[\"mudanca\"])\n",
    "        if \"futuro2\" in nome_arquivo:\n",
    "            sufixo = \" (cen√°rio futuro2)\"\n",
    "        elif \"futuro3\" in nome_arquivo:\n",
    "            sufixo = \" (cen√°rio futuro3)\"\n",
    "        else:\n",
    "            sufixo = \"\"\n",
    "        visualizar_mapa(mapas[\"mudanca\"], f\"Mudan√ßa na Distribui√ß√£o{sufixo}\", escala_padronizada=escala_padronizada)\n",
    "    \n",
    "    # Se temos atual e futuro, calcular diferen√ßa manualmente se n√£o existe mapa de mudan√ßa\n",
    "    elif \"atual\" in mapas and \"futuro\" in mapas:\n",
    "        print(f\"\\nüìç Calculando mapa de diferen√ßa entre distribui√ß√£o futura e atual\")\n",
    "        visualizar_mapa_diferenca(mapas[\"atual\"], mapas[\"futuro\"], escala_padronizada=escala_padronizada)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
